{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LASER documentation","text":"<p>LASER (Light Agent Spatial modeling for ERadication) is a high-performance, agent-based simulation framework for modeling the spread of infectious diseases. It supports spatial structure, age demographics, and modular disease logic using Python-based components.</p> <p>The LASER framework is designed to be flexible. The basis of the framework, <code>laser-core</code>, is composed of modular components that can be used to create custom epidemiological models. For those who want to explore disease dynamics without the need to code from scratch, the development team is creating pre-built models, which will include a generic epidemiological model and disease-specific models. These pre-built models range from simple compartmental models to more complex agent-based models with spatial dynamics. And finally, for those who want to contribute to code, the framework is open source and contributions are welcome!</p>"},{"location":"#learn-more","title":"Learn more","text":"<ul> <li> <p> Get started modeling</p> <p>Create and run simulations.</p> <p> Get started modeling</p> </li> <li> <p> Reference</p> <p>Full details on all classes and functions.</p> <p> API reference</p> </li> <li> <p> Tutorials</p> <p>An interactive tour of key features.</p> <p> Tutorials</p> </li> <li> <p> Glossary</p> <p>Look up unfamiliar terms.</p> <p> Glossary</p> </li> <li> <p> What's new</p> <p>See what's in the latest releases.</p> <p> What's new</p> </li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Christopher Lorton - https://www.idmod.org</li> <li>Jonathan Bloedow - https://www.idmod.org</li> <li>Katherine Rosenfeld - https://www.idmod.org</li> <li>Kevin McCarthy - https://www.idmod.org</li> </ul>"},{"location":"contributions/","title":"Contributing","text":"<p>LASER is an open-source model, and we welcome code contributions, feature requests, documentation improvement requests, and identification of bugs. Every little bit helps, and credit will always be given.</p>"},{"location":"contributions/#bug-reports","title":"Bug reports","text":"<p>When reporting a bug please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributions/#documentation-improvements","title":"Documentation improvements","text":"<p>LASER could always use more documentation, whether as part of the official LASER docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributions/#feature-requests-and-feedback","title":"Feature requests and feedback","text":"<p>The best way to send feedback is to file an issue in the LASER GitHub repo.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how the feature would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that code contributions are welcome! </li> </ul>"},{"location":"contributions/#development","title":"Development","text":"<p>If you develop new features for <code>laser</code>, fix bugs you've found in the model, or want to contribute a new disease type to the LASER framework, we would love to add your code.</p>"},{"location":"contributions/#set-up-your-local-environment","title":"Set up your local environment","text":"<ol> <li> <p>Fork LASER-core using the \"Fork\" button at the top right of the window.</p> </li> <li> <p>Clone your fork locally:</p> <p><code>git clone git@github.com:YOURGITHUBNAME/laser.git</code></p> </li> <li> <p>Install <code>uv</code> in your system [Python], i.e. before creating and activating a virtual environment.</p> </li> <li> <p>Install <code>tox</code> as a tool in <code>uv</code> with the <code>tox-uv</code> plugin with     <code>uv tool install tox --with tox-uv</code></p> </li> <li>Change to the <code>laser-core</code> directory with     <code>cd laser-core</code></li> <li>Create a virtual environment for development with     <code>uv venv</code></li> <li> <p>Activate the virtual environment with</p> <ul> <li> <p>Mac or Linux:   <code>source .venv/bin/activate</code></p> </li> <li> <p>Windows:   <code>.venv\\bin\\Activate</code></p> </li> </ul> </li> <li> <p>Create a branch for local development:</p> <p><code>git checkout -b &lt;name-of-your-bugfix-or-feature&gt;</code> Now you can make your changes locally.</p> </li> <li> <p>Run all the checks and docs builder after completing your changes:</p> <p><code>tox</code></p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <p><code>git add .</code></p> <p><code>git commit -m \"Your detailed description of your changes.\"</code></p> <p><code>git push origin &lt;name-of-your-bugfix-or-feature&gt;</code></p> </li> <li> <p>Submit a pull request through GitHub, following our pull request guidelines:</p> <ul> <li>Include passing tests (run <code>tox</code>)</li> <li>Update the documentation for new API, functionality, etc</li> <li>Add a note to <code>CHANGELOG.md</code> about the changes</li> <li>Add yourself to <code>AUTHORS.md</code></li> </ul> </li> </ol>"},{"location":"contributions/#pull-request-guidelines","title":"Pull request guidelines","text":"<p>If you need some code review or feedback while you're developing the code just make the pull request.</p> <p>For merging, you should:</p> <ol> <li>Include passing tests (run <code>tox</code>).</li> <li>Update documentation when there's new API, functionality etc.</li> <li>Add a note to <code>CHANGELOG.md</code> about the changes.</li> <li>Add yourself to <code>AUTHORS.md</code>.</li> </ol>"},{"location":"contributions/#run-tests","title":"Run tests","text":"<p>Now you can run tests in the <code>tests</code> directory or run the entire check+docs+test suite with <code>tox</code>. Running <code>tox</code> will run several consistency checks, build documentation, run tests against the supported versions of Python, and create a code coverage report based on the test suite. Note that the first run of <code>tox</code> may take a few minutes (~5). Subsequent runs should be quicker depending on the speed of your machine and the test suite (~2 minutes). You can use <code>tox</code> to run tests against a single version of Python with, for example, <code>tox -e py310</code>.</p> <p>To run a subset of tests:</p> <pre><code>tox -e envname -- pytest -k test_myfeature\n</code></pre> <p>To run all the test environments in parallel:</p> <p><pre><code>tox -p auto\n</code></pre> Note, to combine the coverage data from all the tox environments run:</p> <ul> <li>For Windows     <pre><code>set PYTEST_ADDOPTS=--cov-append\ntox\n</code></pre></li> <li>For other operating systems     <pre><code>PYTEST_ADDOPTS=--cov-append tox\n</code></pre></li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"installation/","title":"Installation and software requirements","text":"<p>LASER was developed to fulfill a variety of modeling needs. <code>Laser-core</code> can be installed standalone with:</p> <pre><code>pip install laser-core\n</code></pre> <p>Install <code>laser-core</code> if you are interested in using the components to develop your own, customized model. More information on modeling with <code>laser-core</code> can be found in Creating custom models.</p>"},{"location":"installation/#laser-disease-models","title":"LASER disease models","text":"<p>If you are interested in exploring model dynamics with <code>laser-generic</code> or other disease-specific models, you can find installation and user instructions in Laser disease models.</p>"},{"location":"installation/#laser-development","title":"LASER development","text":"<p>If you are interested in contributing to code, instructions on how to install for development can be found in the Contribution guide.</p>"},{"location":"installation/#set-up-an-ai-interface","title":"Set up an AI interface","text":""},{"location":"parameters/","title":"API reference","text":""},{"location":"whatsnew/","title":"What's new","text":""},{"location":"whatsnew/#001-2023-11-18","title":"0.0.1 (2023-11-18)","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"get-started/","title":"Get started modeling","text":""},{"location":"get-started/#pre-built-laser-disease-models","title":"Pre-built LASER disease models","text":""},{"location":"get-started/#create-custom-models-with-laser-core","title":"Create custom models with LASER-Core","text":""},{"location":"get-started/calibration/","title":"Calibrate custom models","text":"<p>LASER can be calibrated using Optuna. Calibration is a process of tuning model parameters to fit real-world data, to ensure that model output provides accurate insights. Calibration can also be used as a method to help debug your model, as an inability to recreate known phenomena can help pinpoint issues in model code. For more information on calibration, check out IDM's ModelingHub.</p>"},{"location":"get-started/calibration/#simple-local-calibration","title":"Simple local calibration","text":"<ol> <li> <p>Expose parameters in your model. Ensure your LASER model can load and apply parameters you wish to calibrate. These are typically passed through a <code>params</code> dictionary or a <code>PropertySet</code> and might include:</p> <ul> <li>Basic reproduction number (R0)</li> <li>Duration of infection</li> <li>Seeding prevalence</li> </ul> </li> <li> <p>Write post-processing code. Modify your model to save key outputs (e.g., number of infected individuals over time) to a CSV file. For example, use:</p> <p><code>save_results_to_csv(sim.results)</code></p> <p>This CSV will be used later by the objective function.</p> </li> <li> <p>Create the objective function. Write a Python script, usually named <code>objective.py</code>, containing a function like this:</p> <pre><code>def objective(trial):\n    # Load trial parameters\n    R0 = trial.suggest_float(\"R0\", 1.0, 3.5)\n\n    # Run model (via subprocess, or function call)\n    run_model(R0)\n\n    # Load model output and reference data\n    model_df = pd.read_csv(\"output.csv\")\n    ref_df = pd.read_csv(\"reference.csv\")\n\n    # Compare and return score\n    error = np.mean((model_df[\"I\"] - ref_df[\"I\"])**2)\n    return error\n</code></pre> <p>Tip: You can write unit tests for your objective function by mocking model outputs.</p> </li> <li> <p>Test the objective function standalone. Before integrating with Optuna, run your objective function directly to ensure it works:</p> <pre><code>from objective import objective\nfrom optuna.trial import FixedTrial\n\nscore = objective(FixedTrial({\"R0\": 2.5}))\nprint(f\"Test score: {score}\")\n</code></pre> <p>Expected result: A numeric score. If it crashes, check CSV paths and data types.</p> </li> <li> <p>Run simple calibration (SQLite, no Docker). Use the calib/worker.py helper to run a local test study with a small number of trials.</p> <p>Linux/macOS (Bash or similar):</p> <pre><code>export STORAGE_URL=sqlite:///example.db &amp;&amp; python3 calib/worker.py --num-trials=10\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>$env:STORAGE_URL=\"sqlite:///example.db\"; python calib/worker.py --num-trials=10\n</code></pre> <p>This is helpful for debugging. Consider running a scaled-down version of your model to save time.</p> </li> </ol>"},{"location":"get-started/calibration/#local-dockerized-calibration","title":"Local Dockerized calibration","text":"<ol> <li> <p>Dockerize your model and objective. Use the provided <code>Dockerfile</code> to build a container that includes both your model and objective function. Do this from the main directory.</p> <p><code>docker build . -f calib/Dockerfile -t idm-docker-staging.packages.idmod.org/laser/laser-polio:latest</code></p> </li> <li> <p>Create Docker network. You\u2019ll need a shared network so your workers and database container can communicate:</p> <p><code>docker network create optuna-network</code></p> </li> <li> <p>Launch MySQL database container:</p> <pre><code>docker run -d --name optuna-mysql --network optuna-network -p 3306:3306 \\\n  -e MYSQL_ALLOW_EMPTY_PASSWORD=yes \\\n  -e MYSQL_DATABASE=optuna_db mysql:latest\n</code></pre> </li> <li> <p>Launch calibration worker:</p> <pre><code>docker run --rm --name calib_worker --network optuna-network \\\n  -e STORAGE_URL=\"mysql://root@optuna-mysql:3306/optuna_db\" \\\n  idm-docker-staging.packages.idmod.org/laser/laser-polio:latest \\\n  --study-name test_polio_calib --num-trials 1\n</code></pre> <p>If that works, you can change the study name or number of trials.</p> <p>Troubleshooting: If this fails, try running the worker interactively and debug inside:</p> <p><code>docker run -it --network optuna-network --entrypoint /bin/bash idm-docker-staging.packages.idmod.org/laser/laser-polio:latest</code></p> </li> <li> <p>Monitor calibration progress:</p> <p>Use Optuna CLI. You should be able to pip install optuna.</p> <pre><code>optuna trials \\\n  --study-name=test_polio_calib \\\n  --storage \"mysql+pymysql://root:@localhost:3306/optuna_db\"\n\noptuna best-trial \\\n  --study-name=test_polio_calib \\\n  --storage \"mysql+pymysql://root:@localhost:3306/optuna_db\"\n</code></pre> </li> </ol>"},{"location":"get-started/calibration/#cloud-calibration","title":"Cloud calibration","text":"<ol> <li> <p>Push Docker image to registry. If you\u2019ve built a new docker image, you\u2019ll want to push it so it\u2019s available to AKS:</p> <p><code>docker push idm-docker-staging.packages.idmod.org/laser/laser-polio:latest</code></p> </li> <li> <p>Cloud deployment. This step assumes you have secured access to an Azure Kubernetes Service (AKS) cluster. You may need to obtain or generate a new kube config file. Detailed instructions for that are not included here. This step assumes the cluster corresponding to your config is up and accessible.</p> <p><code>cd calib/cloud</code></p> <p>Edit <code>cloud_calib_config.py</code> to set the <code>storage_url</code> to:</p> <p><code>\"mysql+pymysql://optuna:superSecretPassword@localhost:3306/optunaDatabase\"</code></p> <p>Set the study name and number of trials per your preference. Detailed documentation of the other parameters is not included here.</p> <p>Launch multiple workers:</p> <p><code>python3 run_calib_workers.py</code></p> </li> <li> <p>View final results:</p> <p>Forward port to local machine. Note that is the first instruction to rely on installing <code>kubectl</code>. Open a bash shell if necessary.</p> <p><code>kubectl port-forward mysql-0 3306:3306 &amp;</code></p> <p>Use Optuna CLI to check results:</p> <pre><code>optuna trials \\\n  --study-name=test_polio_calib \\\n  --storage \"mysql+pymysql://optuna:superSecretPassword@localhost:3306/optunaDatabase\"\n\noptuna best-trial \\\n  --study-name=test_polio_calib \\\n  --storage \"mysql+pymysql://optuna:superSecretPassword@localhost:3306/optunaDatabase\"\n</code></pre> <p>Generate a report on disk about the study (can be run during study or at end).</p> <p><code>python3 report_calib_aks.py</code></p> <p>Launch Optuna dashboard:</p> <p><code>python -c \"import optuna_dashboard; optuna_dashboard.run_server('mysql+pymysql</code></p> </li> </ol>"},{"location":"get-started/complexity/","title":"Add model complexity","text":""},{"location":"get-started/complexity/#vital-dynamics","title":"Vital dynamics","text":""},{"location":"get-started/complexity/#add-spatial-features","title":"Add spatial features","text":""},{"location":"get-started/complexity/#add-migration","title":"Add migration","text":""},{"location":"get-started/custom/","title":"Create custom models","text":""},{"location":"get-started/custom/#assemble-componentsbuilding-a-model","title":"Assemble components/building a model","text":""},{"location":"get-started/custom/#manually","title":"Manually","text":""},{"location":"get-started/custom/#set-up-an-ai-interface","title":"Set up an AI interface","text":"<p>The modular structure of LASER was designed to ultimately be use with AI.</p> <p>For internal IDM users, you can use a pre-built AI interface, JENNER-GPT to create your simulations.</p> <p>For external users, while we cannot currently grant you access to internal tools, we can help you create your own AI interface.</p>"},{"location":"get-started/custom/#run-simulations","title":"Run simulations","text":""},{"location":"get-started/custom/#visualize-output","title":"Visualize output","text":""},{"location":"get-started/initialization/","title":"Load data and initialize populations","text":""},{"location":"get-started/initialization/#load-data","title":"Load data","text":""},{"location":"get-started/initialization/#initialize-populations","title":"Initialize populations","text":""},{"location":"get-started/initialization/#squash-save-and-load","title":"Squash, save, and load","text":"<p>As the number agents in your LASER population model grows (e.g., 1e8), it can become computationally expensive and unnecessary to repeatedly run the same initialization routine every sim. In many cases -\u2014 particularly during model calibration -\u2014 it is far more efficient to initialize the population once, save it, and then reload the initialized state for subsequent runs.</p> <p>This approach is especially useful when working with EULAs \u2013 Epidemiologically Uninteresting Light Agents. For example, it can be a very powerful optimization to compress all the agents who are already (permanently) recovered or immune in a measles or polio model into a number/bucket. In such models, the majority of the initial population may be in the \u201cRecovered\u201d state, potentially comprising 90% or more of all agents. If you are simulating 100 million agents, storing all of them can result in punitive memory usage.</p> <p>To address this, LASER supports a squashing process. Squashing involves defragmenting the data frame such that all epidemiologically active or \u201cinteresting\u201d agents (e.g., Susceptible or Infectious) are moved to the beginning of the array or table, and less relevant agents (e.g., Recovered) are moved to the end. Though please note that you should assume that squashed agent data is overwritten.</p> <p>Some notes about squashing:</p> <ul> <li>The population count is adjusted so that all for loops and step functions iterate only over the active population.</li> <li>This not only reduces memory usage but also improves performance by avoiding unnecessary computation over inactive agents.</li> </ul> <p>Some notes about using saved populations:</p> <ul> <li>You will want to be confident that the saved population is sufficiently randomized and representative of your overall population.</li> <li>If you are calibrating parameters used to create the initial population in the first place, you\u2019ll need to recreate those parts of the population after loading, diminishing the benefit of the save/load approach.</li> <li> <p>When saving a snapshot, note that only the active (unsquashed) portion of the population is saved. Upon reloading:</p> <ul> <li>Only this subset is allocated in memory.</li> <li>This prevents the performance penalty of managing large volumes of unused agent data.</li> </ul> </li> </ul> <p>Note</p> <p>Before squashing, you should count and record the number of recovered (or otherwise squashed) agents. This count should be stored in a summary variable \u2014- typically the R column of the results data frame. This ensures your model retains a complete epidemiological record even though the agents themselves are no longer instantiated.</p> <p>Procedure:</p> <ol> <li> <p>Add squashing:</p> <ul> <li>Add a <code>squash_recovered()</code> function. This should call <code>LaserFrame.squash(\u2026)</code> with a boolean mask that includes non-recovered agents (disease_state != 2). You may choose a different criterion, such as age-based squashing.</li> <li>Count your \u201csquashed away\u201d agents first. You must compute and store all statistics related to agents being squashed before the <code>squash()</code> call. After squashing, only the left-hand portion of the arrays (up to .count) remains valid.</li> <li>Seed infections after squashing. If your model seeds new infections (disease_state == 1), this must happen after squashing. Otherwise, infected agents may be inadvertently removed.</li> <li>Store the squashed-away totals by node. Before squashing, compute and record node-wise totals (e.g., recovered counts) in <code>results.R[0, :]</code> so this pre-squash information persists.</li> <li>(Optionally) simulate EULA effects once and save. If modeling aging or death among squashed agents, simulate this up front and store the full <code>[time, node]`` matrix (e.g.,</code>results.R[:, :]`). This avoids recomputation at runtime.</li> </ul> </li> <li> <p>Save function: implement a <code>save(path)</code> method:</p> <ul> <li>Use <code>LaserFrame.save_snapshot(path, results_r=..., pars=...)</code></li> <li>Include:<ul> <li>The squashed population (active agents only)</li> <li>The <code>results.R</code> matrix containing both pre-squash and live simulation values</li> <li>The full parameter set in a <code>PropertySet</code></li> </ul> </li> </ul> </li> <li> <p>Load function: implement a <code>load(path)</code> class method:</p> <ul> <li>Call <code>LaserFrame.load_snapshot(path)</code> to retrieve:<ul> <li>Population frame</li> <li>Results matrix</li> <li>Parameters</li> </ul> </li> <li>Set <code>.capacity = .count</code> if not doing births, else set capacity based on projected population growth from count.</li> <li>Reconstruct all components using <code>init_from_file()</code></li> </ul> <p>Warning</p> <p>When modeling vital dynamics, especially births, there is an additional step needed to ensure consistency after loading:</p> <p>Property initialization for unborn individuals must be repeated if your model pre-assigns properties up to <code>.capacity</code>. For example, if timers or demographic attributes (like <code>date_of_birth</code>) are pre-initialized at <code>t=0</code>, you must ensure this initialization is re-applied after loading, because only the <code>.count</code> population is reloaded, not the future <code>.capacity</code>.</p> <p>Failing to do so may result in improperly initialized agents being birthed after the snapshot load, which can lead to subtle or catastrophic model errors.</p> </li> <li> <p>Preserve EULA'd results:</p> <p>Use \"+=\" to track new recoveries alongside pre-squash R values. In <code>run()</code>, use additive updates so that pre-saved recovered agents are preserved:</p> <pre><code>self.results.R[t, nid] += ((self.population.node_id == nid) &amp;\n                       (self.population.disease_state == 2)).sum()\n</code></pre> <p>This ensures your output accounts for both squashed-away immunity and recoveries during the live simulation.</p> </li> </ol> Code example: Add squashing and snapshot support to SIR models <pre><code>import numpy as np\nimport click\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom laser_core import LaserFrame, PropertySet\n\nclass Transmission:\n    \"\"\"\n    A simple transmission component that spreads infection within each node.\n    \"\"\"\n    def __init__(self, population, pars):\n        self.population = population\n        self.pars = pars\n\n    def step(self):\n    \"\"\"\n    For each node in the population, calculate the number of new infections as a function of:\n    - the number of infected individuals,\n    - the number of susceptibles,\n    - adjustments for migration and seasonality,\n    - and individual-level heterogeneity.\n\n    Then, select new infections at random from among the susceptible individuals in each node,\n    and initiate infection in those individuals.\n    \"\"\"\n    pass  # Implementation omitted for documentation purposes\n\n    @classmethod\n    def init_from_file(cls, population, pars):\n        return cls(population, pars)\n\nclass Progression:\n    \"\"\"\n    A simple progression component that recovers infected individuals probabilistically.\n    \"\"\"\n    def __init__(self, population, pars):\n        self.population = population\n        self.pars = pars\n\n    def step(self):\n    \"\"\"\n    At each time step, update the disease state of infected individuals based on the model's\n    progression logic. This may be driven by probabilities, timers, or other intrahost dynamics.\n    \"\"\"\n    pass  # Implementation omitted for documentation\n\n    @classmethod\n    def init_from_file(cls, population, pars):\n        return cls(population, pars)\n\nclass RecoveredSquashModel:\n    \"\"\"\n    A simple multi-node SIR model demonstrating use of LASER's squash and snapshot mechanisms.\n    \"\"\"\n    def __init__(self, num_agents=100000, num_nodes=20, timesteps=365):\n        self.num_agents = num_agents\n        self.num_nodes = num_nodes\n        self.timesteps = timesteps\n        self.population = LaserFrame(capacity=num_agents, initial_count=num_agents)\n        self.population.add_scalar_property(\"node_id\", dtype=np.int32)\n        self.population.add_scalar_property(\"disease_state\", dtype=np.int8)  # 0=S, 1=I, 2=R\n\n        self.results = LaserFrame(capacity=self.num_nodes)\n        self.results.add_vector_property(\"S\", length=timesteps, dtype=np.int32)\n        self.results.add_vector_property(\"I\", length=timesteps, dtype=np.int32)\n        self.results.add_vector_property(\"R\", length=timesteps, dtype=np.int32)\n\n        self.pars = PropertySet({\n            \"r0\": 2.5,\n            \"migration_k\": 0.1,\n            \"seasonal_factor\": 0.8,\n            \"transmission_prob\": 0.2,\n            \"recovery_days\": 14\n        })\n\n        self.components = [\n            Transmission(self.population, self.pars),\n            Progression(self.population, self.pars)\n            # could add other components like vaccination\n        ]\n\n    def initialize(self):\n        np.random.seed(42)\n        self.population.node_id[:] = np.random.randint(0, self.num_nodes, size=self.num_agents)\n        recovered = np.random.rand(self.num_agents) &lt; 0.6\n        self.population.disease_state[:] = np.where(recovered, 2, 0)\n\n    def seed_infections(self):\n        susceptible = self.population.disease_state == 0\n        num_seed = max(1, int(0.001 * self.population.count))\n        seed_indices = np.random.choice(np.where(susceptible)[0], size=num_seed, replace=False)\n        self.population.disease_state[seed_indices] = 1\n\n    def squash_recovered(self):\n        \"\"\"\n        Removes all agents who are recovered (state 2).\n        This reduces memory footprint and speeds up simulation.\n        \"\"\"\n        keep = self.population.disease_state[:self.population.count] != 2\n        self.population.squash(keep)\n\n    def populate_results(self):\n        \"\"\"\n        Populate initial R values before squashing to reflect the pre-squash immunity landscape.\n        \"\"\"\n        for nid in range(self.num_nodes):\n            initial_r = ((self.population.disease_state == 2) &amp; (self.population.node_id == nid)).sum()\n            decay = np.linspace(initial_r, initial_r * 0.9, self.timesteps, dtype=int)\n            self.results.R[:, nid] = decay\n        print(\"Initial R counts per node:\", self.results.R[0, :])\n        print(\"Total initial R (summed):\", self.results.R[0, :].sum())\n\n    def run(self):\n        for t in range(self.timesteps):\n            for component in self.components:\n                component.step()\n            for nid in range(self.num_nodes):\n                self.results.S[t, nid] = ((self.population.node_id == nid) &amp; (self.population.disease_state == 0)).sum()\n                self.results.I[t, nid] = ((self.population.node_id == nid) &amp; (self.population.disease_state == 1)).sum()\n                self.results.R[t, nid] += ((self.population.node_id == nid) &amp; (self.population.disease_state == 2)).sum()\n\n    def save(self, path):\n        \"\"\"\n        Save the current model state to an HDF5 file, including population frame,\n        pre-squash results, and simulation parameters.\n        \"\"\"\n        self.population.save_snapshot(path, results_r=self.results.R, pars=self.pars)\n\n    @classmethod\n    def load(cls, path):\n        \"\"\"\n        Reload a model from an HDF5 snapshot. Note: reloaded population will have\n        only post-squash agents (e.g., susceptibles and infected).\n        \"\"\"\n        pop, results_r, pars = LaserFrame.load_snapshot(path)\n        model = cls(num_agents=pop.capacity, num_nodes=results_r.shape[1], timesteps=results_r.shape[0])\n        model.population = pop\n        model.results.R[:, :] = results_r\n        model.pars = PropertySet(pars)\n        model.pars[\"transmission_prob\"] /= 10  # example modification after reload\n        model.components = [\n            Transmission.init_from_file(model.population, model.pars),\n            Progression.init_from_file(model.population, model.pars)\n        ]\n        return model\n\n    def plot(self):\n        \"\"\"\n        Plot the time series of total S, I, and R across all nodes.\n        \"\"\"\n        # details omitted\n\n@click.command()\n@click.option(\"--init-pop-file\", type=click.Path(), default=None, help=\"Path to snapshot to resume from.\")\n@click.option(\"--output\", type=click.Path(), default=\"model_output.h5\")\ndef main(init_pop_file, output):\n    if init_pop_file:\n        model = RecoveredSquashModel.load(init_pop_file)\n        model.run()\n        model.plot()\n    else:\n        model = RecoveredSquashModel()\n        model.initialize()\n        model.seed_infections()\n        model.populate_results()\n        model.squash_recovered()\n        model.save(output)\n        print(f\"Initial population saved to {output}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"get-started/optimization/","title":"Optimize custom models","text":"<p>As an agent-based model, research using LASER will require thousands of simulation replicates. While the model is designed to perform well with large agent populations, there is still a need to utilize high compute power and to optimize model performance.</p> <p>When creating custom models, knowing how to identify and fix performance bottlenecks can save compute time and speed results.</p>"},{"location":"get-started/optimization/#identifying-bottlenecks","title":"Identifying bottlenecks","text":"<p>Typically, we do not recommend running the code through a profiler, at least not initially. Instead, we take advantage of LASER\u2019s highly modular structure and AI-driven optimization.</p> <p>The first step is to add simple timing code that tracks the total time spent in each component over a realistic simulation. Then, plot a pie chart at the end to visualize where the most time is spent. A simple way to track execution time is using the <code>time package</code>.</p> Code example: Identify bottlenecks <pre><code>def run(self):\n    self.component_times = {component.__class__.__name__: 0 for component in self.components}\n    self.component_times[\"reporting\"] = 0\n    for tick in tqdm(range(self.pars.timesteps)):\n        for component in self.components:\n            start_time = time.time()  # Start timing for the component\n            component.step()\n            end_time = time.time()  # End timing for the component\n\n            # Accumulate the time taken for this component\n            elapsed_time = end_time - start_time\n            component_name = component.__class__.__name__\n</code></pre> <p>This often reveals the top 1 to 3 performance bottlenecks. Focus first on the biggest offender\u2014it provides the most opportunity for speedup. Often, the largest bottleneck is not what you might instinctively expect. Avoid optimizing a component only to find out it contributes just a small percentage of the total runtime. A modest improvement in the runtime of an \u201cexpensive\u201d component is often more effective than spending a lot of time on highly optimizing a component which only accounts for a small fraction of runtime. Also, make sure that your reporting code is being measured and reported, ideally in its own \u2018bucket\u2019. This may be easier or harder depending on how you are doing reporting. Since reporting usually involves counting over the entire population, it usually shows up as a hotspot sooner or later. Fortunately, it\u2019s usually fairly easy to speed up. Or even eliminate.</p>"},{"location":"get-started/optimization/#leverage-ai","title":"Leverage AI","text":"<p>Once you have identified the slowest component, the easiest way to improve performance is by using ChatGPT. Try prompting with:</p> <p><code>\"This code is much too slow. (My arrays are all about 1e6 or 1e7 in size.)\"</code></p> <p>If your code consists mainly of for-loops without much NumPy, you can add:</p> <p><code>\"Is there anything we can vectorize better with NumPy?\"</code></p> <p>This approach can often transform a naive implementation into a highly optimized one.</p>"},{"location":"get-started/optimization/#implement-unit-tests","title":"Implement unit tests","text":"<p>Instead of testing performance within the full simulation, consider building unit tests. This ensures correctness while optimizing for speed.</p> <ul> <li>Use AI to generate unit tests that validate output against a known correct (but slower) version.</li> <li>Include performance benchmarks in the tests.</li> <li>Ensure large array sizes (e.g., 1 million+ elements) to get meaningful speed comparisons.</li> </ul>"},{"location":"get-started/optimization/#optimize-with-numpy-and-numba","title":"Optimize with NumPy and Numba","text":"<p>After achieving good performance with NumPy, consider trying Numba for further improvements.</p> <p>Even if you\u2019re new to Numba, ChatGPT can generate optimized solutions easily. Keep in mind:</p> <ul> <li>Numba moves back to explicit for-loops (unlike NumPy, which uses vectorization syntax).</li> <li>GPT\u2019s first solution may use <code>range</code> instead of <code>prange</code>. Prompt it with:     <code>\"Can we parallelize this with prange?\"</code></li> <li>If your code involves common counters, atomic operations may become a bottleneck. Ask GPT about:     <code>\"Can we use thread-local storage to avoid atomic operations?\"</code></li> <li>Numba may be slower than NumPy for small arrays (e.g., thousands or tens of thousands of elements). Test with at least 1 million elements.</li> </ul>"},{"location":"get-started/optimization/#c-and-openmp","title":"C and OpenMP","text":"<p>If the best Numba solution still isn\u2019t fast enough, consider compiled C.</p> <ul> <li>Use ctypes to call C functions from Python.</li> <li>Mention \u201cuse OpenMP\u201d in AI prompts if parallelization is possible.</li> <li>Ask: <code>\"Can you generate an OpenMP solution with the best pragmas?\"</code></li> <li>The more CPU cores available, the greater the potential speedup. That said, it\u2019s usually a case of diminishing returns as one goes from 8 cores to 16 and to 32. Our research shows that often you\u2019re better off running 4 sims across 8 cores each than running 1 sim on all 32 cores available. Also be aware that with both Numba and OpenMP you can constrain the number of cores used to less than the number available by setting the appropriate environment variable. (Numba environment variable = NUMBA_NUM_THREADS; OpenMP environment variable = OMP_NUM_THREADS)</li> </ul>"},{"location":"get-started/optimization/#additional-advice","title":"Additional advice","text":"<ul> <li> <p>Don\u2019t duplicate. Sometimes reporting will duplicate transmission code and need to be combined.</p> </li> <li> <p>Never append. There may be cases where you are collecting information as it happens without knowing ahead of time how many rows/entries/elements you\u2019ll need. This is easy in Python using list appending, for example, but that\u2019s a performance killer. Really try to find a way to figure out ahead of time how many entries there will be, and then allocate memory for that, and insert into the existing row.</p> </li> <li> <p>Some components have long time-scales, like mortality. By default you are probably going to end up doing most component steps every timestep. You can probably get away with doing mortality updates, for example, far less often. You can experiement with weekly, fortnightly or monthly updates, depending on the timescale of the component you\u2019re optimizing. Just be sure to move everything forward by a week if you\u2019re only doing the update every week. And expect \u201cblocky\u201d plots. Note that there are fancier solutions like \u2018strided sharding\u2019 (details omitted).</p> </li> <li> <p>When prompting AI, use questions rather than directives. For example:</p> <p><code>\"Do you think it might be better to...?\"</code></p> <p>This prevents oversteering the AI into suboptimal solutions.</p> </li> </ul>"},{"location":"software-overview/","title":"Software overview","text":"<p>LASER is a modeling framework that includes a variety of ways for users to implement the code to model infectious diseases. At the root of the framework is <code>laser-core</code>, a suite of components that can be assembled or customized to fit specific modeling needs. The LASER development team is also in the process of producing pre-built disease models crafted from <code>laser-core</code> components, which are tailored to address specific public health modeling questions. You can learn more about creating custom models using <code>laser-core</code> or running models using pre-built disease models in the Get started modeling section.</p>"},{"location":"software-overview/#design-principles","title":"Design principles","text":"<p>The philosophy driving the development of LASER was to create a framework that was flexible, powerful, and fast, able to tackle a variety of complex modeling scenarios without sacrificing performance. But complexity often slows performance, and not every modeling question requires a full suite of model features. To solve this problem, LASER was designed as a set of core components, each with fundamental features that could be added--or not--to build working models. Users can optimize performance by creating models tailored to their research needs, only using components necessary for their modeling question. This building-block framework enables parsimony in model design, but also facilitates the building of powerful models with bespoke, complex dynamics.</p>"},{"location":"software-overview/#software-architecture","title":"Software architecture","text":""},{"location":"software-overview/#input-and-output-files","title":"Input and output files","text":""},{"location":"software-overview/#software-components","title":"Software components","text":"<p>Components are modular units of functionality within the simulation, responsible for performing specific updates or computations on the agent population or node-level data. Each component is implemented as a class with an initialization function to set up any required state and a step function to execute the component\u2019s logic during each timestep.</p>"},{"location":"software-overview/database/","title":"Simulation data and properties","text":""},{"location":"software-overview/migration/","title":"Migration","text":"<p>The ability to add spatial dynamics to LASER models is one of the features that makes the framework so powerful. There are multiple methods available for adding migration to your model, and the class you choose will depend on which features are important for your research question. Each of the migration models will distribute your population of agents among a set of nodes, with set distances between nodes, and utilize a matrix to define the connection between nodes. How agents or infectivity travels between the nodes (and which nodes they may travel to) will be determined by the specific migration model you choose.</p>"},{"location":"software-overview/migration/#sequential-migration-matrix","title":"Sequential migration matrix","text":""},{"location":"software-overview/migration/#gravity-model","title":"Gravity model","text":"<p>The gravity model can be used to compute the migration of people between nodes located at specific distances, with migration rates proportional to population size and the distance between nodes. This type of migration is useful when you would like to add 2-dimensional movement of agents to nodes.</p> <p>Functional form:</p> \\[ M_{ij} = k \\frac{P_i^{a} P_j^{b}}{d_{ij}^{c}} \\] <p>Where:</p> <ul> <li>\\(M_{ij}\\) = migration flow from origin i to destination j</li> <li>\\(P_i, P_j\\) = populations of origin and destination</li> <li>\\(d_{ij}\\) = distance between i and j</li> <li>\\(k\\) = Parameter: scaling constant</li> <li>\\(a\\) = Parameter: exponent for the population of the origin</li> <li>\\(b\\) = Parameter: exponent for the population of the destination</li> <li>\\(c\\) = Parameter: exponent for the distance</li> </ul> <p>The following example demonstrates implementing the gravity model to calculate the number of migrants moving between nodes. Agents are randomly assigned to different migration paths.</p> <pre><code>import numpy as np\nfrom laser_core.migration import gravity\n\n# Define populations and distances\npopulations = np.array([5000, 10000, 15000, 20000, 25000])  # Unequal populations\ndistances = np.array([\n    [0.0, 10.0, 15.0, 20.0, 25.0],\n    [10.0, 0.0, 10.0, 15.0, 20.0],\n    [15.0, 10.0, 0.0, 10.0, 15.0],\n    [20.0, 15.0, 10.0, 0.0, 10.0],\n    [25.0, 20.0, 15.0, 10.0, 0.0]\n])\n\n# Gravity model parameters\nk = 0.1    # Scaling constant\na = 0.5    # Exponent for the population of the origin\nb = 1.0    # Exponent for the population of the destination\nc = 2.0    # Exponent for the distance\n\n# Compute the gravity model network\nmigration_network = gravity(populations, distances, k=k, a=a, b=b, c=c)\n\n# Normalize to ensure total migrations represent 1% of the population\ntotal_population = np.sum(populations)\nmigration_fraction = 0.01  # 1% of the population migrates\nscaling_factor = (total_population * migration_fraction) / np.sum(migration_network)\nmigration_network *= scaling_factor\n\n# Generate a node ID array for agents\nnode_ids = np.concatenate([np.full(count, i) for i, count in enumerate(populations)])\n\n# Initialize a 2D array for migration counts\nmigration_matrix = np.zeros_like(distances, dtype=int)\n\n# Select migrants based on the gravity model\nfor origin in range(len(populations)):\n    for destination in range(len(populations)):\n        if origin != destination:\n            # Number of migrants to move from origin to destination\n            num_migrants = int(migration_network[origin, destination])\n            # Select migrants randomly\n            origin_ids = np.where(node_ids == origin)[0]\n            selected_migrants = np.random.choice(origin_ids, size=num_migrants, replace=False)\n            # Update the migration matrix\n            migration_matrix[origin, destination] = num_migrants\n</code></pre>"},{"location":"software-overview/migration/#the-competing-destinations-model","title":"The competing destinations model","text":"<p>The competing destinations model extends the gravity model by incorporating the fact that nodes may not be independent, but instead may \"compete\" with each other for incoming agents. Some nodes may be more or less attractive than other nodes, regardless of their proximity to the origin node. There may  be synergistic or antagonistic effects of nodes, creating specific networks and relationships among a series of nodes.</p> <p>For example, in a \u201csynergistic\u201d version, perhaps migratory flow from Boston to Baltimore is higher than flow between two comparator cities of similar population and at similar distance, because the proximity of Washington, D.C. to Baltimore makes travel to Baltimore more attractive to Bostonians. This would be accounted for by a positive value of \\(\\delta\\). On the other hand, this term may also be \u201cantagonistic\u201d if Washington is such an attractive destination that Bostonians eschew travel to Baltimore entirely; this would indicate a negative value of \\(\\delta\\).</p> <p>Functional form:</p> \\[ M_{i,j} = k \\frac{P_i^{a} P_j^{b}}{d_{ij}^{c}} \\left(\\sum_{k \\neq i,j} \\frac{P_k^{b}}{d_{jk}^{c}}\\right)^{\\delta} \\]"},{"location":"software-overview/migration/#stouffers-rank-model","title":"Stouffer's rank model","text":"<p>Stouffer argued that human mobility patterns do not respond to absolute distance directly, but only indirectly through the accumulation of intervening opportunities for destinations. Stouffer thus proposed a model with no distance-dependence at all, rather only a term that accounts for all potential destinations closer than destination \\(j\\); thus, longer-distance travel depends on the density of attractive destinations at shorter distances.</p> <p>Mathematical formulation:</p> <p>Define \\(\\Omega (i,j)\\) to be the set of all locations \\(k\\) such that \\(D_{i,k} \\leq D_{i,j}\\)</p> \\[ M_{i,j} = kp_i^a \\sum_j \\left(\\frac{p_j}{\\sum_{k \\in D(i,j)} p_k}\\right)^b \\] <p>This presents us with the choice of whether or not the origin population \\(i\\) is included in \\(\\Omega\\) - i.e., does the same \"gravity\" that brings others to visit a community reduce the propensity of that community's members to travel to other communities?</p> <p>The Stouffer model does not include the impact from the local community:</p> \\[ \\Omega(i,j) = \\left(k:0 &lt; D_{i,k} \\leq D_{i,j}\\right). \\] <p>The Stouffer variant model does include the impact of the local community:</p> \\[ \\Omega(i,j) = \\left(k:0 \\leq D_{i,k} \\leq D_{i,j}\\right). \\] <p>To simplify the code, <code>laser-core</code>'s implementation of the Stouffer model includes a parameter <code>include_home</code>.</p>"},{"location":"software-overview/migration/#radiation-model","title":"Radiation model","text":"<p>The radiation model is a parameter-free model (up to an overall scaling constant for total migration flux), derived from arguments around job-related commuting. The radiation model overcomes limitations of the gravity model (which is limited to flow at two specific points and is proportional to the populations at source and destination) by only requiring data on population densities. It can describe situations in which outbound migration flux from origin to destination is enhanced by destination population and absorbed by the density of nearer destinations.</p> <p>Mathematical formulation, whith \\(\\Omega\\) defined as above in the Stouffer model:</p> \\[ M_{i,j} = k \\frac{p_i p_j}{\\left(p_i + \\sum_{k \\in \\theta(i,j)} p_k\\right) \\left(p_i + p_j + \\sum_{k \\in \\theta(i,j)} p_k\\right)} \\] <p>We again use the parameter <code>include_home</code> to determine whether or not location \\(i\\) is to be included in \\(\\Omega(i,j)\\).</p>"},{"location":"software-overview/demographics/","title":"Demographics","text":""},{"location":"software-overview/demographics/#age-structure","title":"Age structure","text":"<p>If you want to work with age structure for a short simulation which doesn\u2019t need births you can just give everyone an age (based on distribution) and increment it each timestep. The <code>laser_core.demographics.pyramid</code> module is provided to support the initialization of agents with plausible initial ages.</p>"},{"location":"software-overview/demographics/#births","title":"Births","text":""},{"location":"software-overview/demographics/#preborn-management-in-laser","title":"Preborn management in LASER","text":"<p>LASER\u2019s design philosophy emphasizes contiguous and fixed-size arrays, meaning all agents\u2014both currently active and preborn\u2014are created at the start of the simulation. Preborns are \u201cactivated\u201d as they are born, rather than being dynamically added. Several approaches to handling preborns while adhering to these principles are outlined below:</p> <p>Negative and Positive Birthdays:</p> <ul> <li>Assign <code>date_of_birth</code> values in the past (negative) for active agents.</li> <li>Assign <code>date_of_birth</code> values in the future (positive) for preborns.</li> </ul> <p>Unified Preborn Marker:</p> <ul> <li>Set all preborns\u2019 <code>date_of_birth</code> to a placeholder value (e.g., -1).</li> <li>Update the <code>date_of_birth</code> to the current timestep when a preborn is born.</li> </ul> <p>Active Flag Only (if not modeling age structure):</p> <ul> <li>If the model doesn\u2019t require age structure, you can skip date_of_birth entirely. Instead, use an active flag. Preborns start with <code>active = False</code> and are switched to <code>active = True</code> during the fertility step. This simplifies implementation while remaining consistent with LASER principles.</li> </ul>"},{"location":"software-overview/demographics/#calculating-age-from-birthday","title":"Calculating age from birthday","text":"<p>If calculating age isn\u2019t frequent or essential, you can avoid explicitly tracking an age property. Instead, compute age dynamically as the difference between the current timestep (now) and <code>date_of_birth</code>. For models that depend on age-specific dynamics (e.g., fertility rates by age group), consider adding a dedicated age property that updates at each timestep.</p>"},{"location":"software-overview/demographics/#deaths","title":"Deaths","text":"<p>The recommended way of doing mortality in LASER is by precalculating a lifespan for each agent, rather than probabilistically killing agents as the simulation runs. This can take different forms: If you prefer to track agent age, you can also have an agent lifespan. Alternatively, if you are just using <code>date_of_birth</code> you can have a <code>date_of_death</code>, where theses \u2018dates\u2019 are really simulation times (\u2018sim day of birth\u2019 and \u2018sim day of death\u2019).</p> <p>In LASER, we strive to leave the contiguous arrays of agent data in place, without adding or deleting elements (allocating or freeing). This means that to model mortality, we prefer to \u2018kill\u2019 agents by doing either:</p> <ol> <li> <p>check that their age is greater than their lifespan (or that the current timestep is greater than their \u2018sim day of death\u2019) in each component that cares, or</p> </li> <li> <p>Set an active flag to \"false\" or a dead flag to \"true.\"</p> </li> </ol> <p>The second approach is simpler, and avoids doing millions of comparison operations, at the cost of an additional property. Note that many component operations (step functions) can be done without checking whether the agent is alive, because, for example, as long as transmission never infects a dead person, decrementing all non-zero infection timers will only operate on live agents.</p> <p>Finally, while you can set lifespans using any algorithm you want, <code>laser_core.demographics.kmestimator</code> is provided to support these calculations.</p>"},{"location":"software-overview/demographics/#population-pyramids","title":"Population pyramids","text":"<p>The <code>AliasedDistribution</code> class provides a way to sample from a set of options with unequal probabilities, e.g., a population pyramid.</p> <p>The input to the <code>AliasedDistribution</code> constructor is an array of counts by bin as we would naturally get from a population pyramid (# of people in each age bin).</p> <p><code>AliasedDistribution.sample()</code> returns bin indices so it is up to the user to convert the values returned from <code>sample()</code> to actual ages.</p> <p>Expected format of the population pyramid CSV file for <code>load_pyramid_csv()</code>:</p> <pre><code>Header: Age,M,F\nstart-end,#males,#females\nstart-end,#males,#females\nstart-end,#males,#females\n\u2026\nstart-end,#males,#females\nmax+,#males,#females\n</code></pre> <p>For example,</p> <pre><code>Age,M,F\n0-4,9596708,9175309\n5-9,10361680,9904126\n10-14,10781688,10274310\n15-19,11448281,10950664\n\u2026\n90-94,757034,1281854\n95-99,172530,361883\n100+,27665,76635\n</code></pre> <p><code>load_pyramid_csv()</code> returns a 4 column NumPy array with the following columns:</p> <pre><code>0 - Lower bound of age bin, inclusive\n1 - Upper bound of age bin, inclusive\n2 - number of males in the age bin\n3 - number of females in the age bin\n</code></pre> Code example: Loading population pyramids from .csv files <pre><code>import numpy as np\nfrom laser_core.demographics import load_pyramid_csv, AliasedDistribution\nimport importlib.util\nimport os\n\nMCOL = 2\nFCOL = 3\n\nMINCOL = 0\nMAXCOL = 1\n\n# Access the bundled file dynamically\nlaser_core_path = importlib.util.find_spec(\"laser_core\").origin\nlaser_core_dir = os.path.dirname(laser_core_path)\npyramid_file = os.path.join(laser_core_dir, \"data/us-pyramid-2023.csv\")\n\npyramid = load_pyramid_csv(pyramid_file)\nsampler = AliasedDistribution(pyramid[:, MCOL])    # We'll use the male population in this example.\nn_agents = 100_000\nsamples = sampler.sample(n_agents)              # Sample 100,000 people from the distribution.\n# samples will be bin indices, so we need to convert them to ages.\nbin_min_age_days = pyramid[:, MINCOL] * 365          # minimum age for bin, in days (include this value)\nbin_max_age_days = (pyramid[:, MAXCOL] + 1) * 365    # maximum age for bin, in days (exclude this value)\nmask = np.zeros(n_agents, dtype=bool)\nages = np.zeros(n_agents, dtype=np.int32)\nfor i in range(len(pyramid)):   # for each possible bin value...\n    mask[:] = samples == i      # ...find the agents that belong to this bin\n    # ...and assign a random age, in days, within the bin\n    ages[mask] = np.random.randint(bin_min_age_days[i], bin_max_age_days[i], mask.sum())\n\n# in some LASER models we convert current ages to dates of birth by negating the age\n# dob = -ages\n</code></pre> <p>To explore working with age pyramids, see Age pyramid examples.</p>"},{"location":"software-overview/demographics/#kaplan-meier-estimators","title":"Kaplan-Meier estimators","text":"<p>The <code>KaplanMeierEstimator</code> is used to predict age or year of death. It takes an array of cumulative deaths and returns an object that will sample from the Kaplan-Meier distribution.</p> <p>A sample input array of cumulative deaths might look like this:</p> <pre><code>cd[0] = 687 # 687 deaths in the first year (age 0)\ncd[1] = 733 # +46 deaths in the second year (age 1)\ncd[2] = 767 # +34 deaths in the third year (age 2)\n...\ncd[100] = 100_000  # 100,000 deaths by end of year\n</code></pre> <p><code>predict_year_of_death()</code> takes an array of current ages (in years) and returns an array of predicted years of death based on the cumulative deaths input array.</p> <p>Note</p> <p><code>predict_year_of_death()</code> can use non-constant width age bins and will return predictions by age bin. In this case, it is up to the user to convert the returned bin indices to actual years.</p> <p>A sample non-constant width age bin input array might look like this:</p> <pre><code>cd[0] = 340 # 1/2 of first year deaths in the first 3 months\ncd[1] = 510 # another 1/4 (+170) of first year deaths in the next 3 months\ncd[2] = 687 # another 1/4 (+177) of first year deaths in the last 6 months\ncd[3] = 733 # 46 deaths in the second year (age 1)\ncd[4] = 767 # 34 deaths in the third year (age 2)\n...\ncd[103] = 100_000  # 100,000 deaths by end of year 100\n</code></pre> <p>In this example, values returned from predict_year_of_death() would need to be mapped as follows:</p> <pre><code>0 -&gt; (0, 3] months\n1 -&gt; (3, 6] months\n2 -&gt; (6, 12] months\n3 -&gt; 1 year\n4 -&gt; 2 years\n...\n102 -&gt; 100 years\n</code></pre> <p><code>predict_age_at_death()</code> takes an array of current ages (in days) and returns an array of predicted ages (in days) at death. The implementation assumes that the cumulative deaths input array to the estimator represents one year age bins. If you are using non-constant width age bins, you should manually convert bin indices returned from <code>predict_year_of_death()</code> to ages.</p> <p>To explore working with Kapaln-Meier estimators, see Non-disease death estimation.</p>"},{"location":"software-overview/demographics/#spatial-distributions-of-populations","title":"Spatial distributions of populations","text":""},{"location":"software-overview/demographics/age_pyramid/","title":"Age pyramid examples","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\nfrom IPython.display import Image\nImage(filename=\"Nigeria-2024.png\")\nImage(filename=\"UnitedStates-2024.png\")\nimport numpy as np\n\nfrom laser_core.demographics import AliasedDistribution\nfrom laser_core.demographics import load_pyramid_csv\n</pre> from pathlib import Path from IPython.display import Image Image(filename=\"Nigeria-2024.png\") Image(filename=\"UnitedStates-2024.png\") import numpy as np  from laser_core.demographics import AliasedDistribution from laser_core.demographics import load_pyramid_csv  In\u00a0[2]: Copied! <pre>MCOL = 2\nFCOL = 3\n\nMINCOL = 0\nMAXCOL = 1\n\nnigeria = load_pyramid_csv(Path.cwd() / \"Nigeria-2024.csv\")\nsampler = AliasedDistribution(nigeria[:, MCOL])    # We'll use the male population in this example.\nn_agents = 100_000\nsamples = sampler.sample(n_agents)              # Sample 100,000 people from the distribution.\n# samples will be bin indices, so we need to convert them to ages.\nbin_min_age_days = nigeria[:, MINCOL] * 365          # minimum age for bin, in days (include this value)\nbin_max_age_days = (nigeria[:, MAXCOL] + 1) * 365    # maximum age for bin, in days (exclude this value)\nmask = np.zeros(n_agents, dtype=bool)\nages = np.zeros(n_agents, dtype=np.int32)\nfor i in range(len(nigeria)):   # for each possible bin value...\n    mask[:] = samples == i      # ...find the agents that belong to this bin\n    # ...and assign a random age, in days, within the bin\n    ages[mask] = np.random.randint(bin_min_age_days[i], bin_max_age_days[i], mask.sum())\n</pre> MCOL = 2 FCOL = 3  MINCOL = 0 MAXCOL = 1  nigeria = load_pyramid_csv(Path.cwd() / \"Nigeria-2024.csv\") sampler = AliasedDistribution(nigeria[:, MCOL])    # We'll use the male population in this example. n_agents = 100_000 samples = sampler.sample(n_agents)              # Sample 100,000 people from the distribution. # samples will be bin indices, so we need to convert them to ages. bin_min_age_days = nigeria[:, MINCOL] * 365          # minimum age for bin, in days (include this value) bin_max_age_days = (nigeria[:, MAXCOL] + 1) * 365    # maximum age for bin, in days (exclude this value) mask = np.zeros(n_agents, dtype=bool) ages = np.zeros(n_agents, dtype=np.int32) for i in range(len(nigeria)):   # for each possible bin value...     mask[:] = samples == i      # ...find the agents that belong to this bin     # ...and assign a random age, in days, within the bin     ages[mask] = np.random.randint(bin_min_age_days[i], bin_max_age_days[i], mask.sum())  In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\n\n# Plot histogram of samples on primary y-axis\n\nfraction = nigeria[:, MCOL]/nigeria[:, MCOL:FCOL+1].sum()\nax1.plot(fraction, color=\"green\")\nax1.set_xlabel(\"Age Bin\")\nbins = [f\"{i, j}\" for i, j in zip(nigeria[:, MINCOL], nigeria[:, MAXCOL])]\nplt.xticks(rotation=90)\nax1.set_xticks(ticks=range(len(bins)), labels=bins)\nax1.set_ylim(-0.01, 0.1)\nax1.set_ylabel(\"% Population (Input)\", color=\"green\")\n\n# Create secondary y-axis\nax2 = ax1.twinx()\n\n# We will calculate the histogram manually because the final bin isn't the same width as the others.\nhistogram = np.zeros(21, dtype=np.int32)\nfor i in range(histogram.shape[0]):\n    mask = (ages // 1825) == i  # Convert ages in days to 5-year bin indices\n    histogram[i] = mask.sum()\n\nscale = nigeria[:,MCOL].sum() / nigeria[:,MCOL:FCOL+1].sum()   # male / male+female\nfraction = (histogram*scale)/histogram.sum()\nax2.plot(range(histogram.shape[0]), fraction, color=\"orange\", marker=\"x\")\nax2.set_ylim(-0.01, 0.1)\nax2.set_ylabel(\"% Population (Sampled)\", color=\"orange\")\n\nplt.title(\"Age Distribution in Nigeria\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, ax1 = plt.subplots()  # Plot histogram of samples on primary y-axis  fraction = nigeria[:, MCOL]/nigeria[:, MCOL:FCOL+1].sum() ax1.plot(fraction, color=\"green\") ax1.set_xlabel(\"Age Bin\") bins = [f\"{i, j}\" for i, j in zip(nigeria[:, MINCOL], nigeria[:, MAXCOL])] plt.xticks(rotation=90) ax1.set_xticks(ticks=range(len(bins)), labels=bins) ax1.set_ylim(-0.01, 0.1) ax1.set_ylabel(\"% Population (Input)\", color=\"green\")  # Create secondary y-axis ax2 = ax1.twinx()  # We will calculate the histogram manually because the final bin isn't the same width as the others. histogram = np.zeros(21, dtype=np.int32) for i in range(histogram.shape[0]):     mask = (ages // 1825) == i  # Convert ages in days to 5-year bin indices     histogram[i] = mask.sum()  scale = nigeria[:,MCOL].sum() / nigeria[:,MCOL:FCOL+1].sum()   # male / male+female fraction = (histogram*scale)/histogram.sum() ax2.plot(range(histogram.shape[0]), fraction, color=\"orange\", marker=\"x\") ax2.set_ylim(-0.01, 0.1) ax2.set_ylabel(\"% Population (Sampled)\", color=\"orange\")  plt.title(\"Age Distribution in Nigeria\") plt.show() In\u00a0[4]: Copied! <pre>america = load_pyramid_csv(Path.cwd() / \"United States of America-2024.csv\")\nsampler = AliasedDistribution(america[:, FCOL])    # We'll use the female population in this example.\nn_agents = 100_000\nsamples = sampler.sample(n_agents)              # Sample 100,000 people from the distribution.\n# samples will be bin indices, so we need to convert them to ages.\nbin_min_age_days = america[:, MINCOL] * 365          # minimum age for bin, in days (include this value)\nbin_max_age_days = (america[:, MAXCOL] + 1) * 365    # maximum age for bin, in days (exclude this value)\nmask = np.zeros(n_agents, dtype=bool)\nages = np.zeros(n_agents, dtype=np.int32)\nfor i in range(len(america)):   # for each possible bin value...\n    mask[:] = samples == i      # ...find the agents that belong to this bin\n    # ...and assign a random age, in days, within the bin\n    ages[mask] = np.random.randint(bin_min_age_days[i], bin_max_age_days[i], mask.sum())\n</pre> america = load_pyramid_csv(Path.cwd() / \"United States of America-2024.csv\") sampler = AliasedDistribution(america[:, FCOL])    # We'll use the female population in this example. n_agents = 100_000 samples = sampler.sample(n_agents)              # Sample 100,000 people from the distribution. # samples will be bin indices, so we need to convert them to ages. bin_min_age_days = america[:, MINCOL] * 365          # minimum age for bin, in days (include this value) bin_max_age_days = (america[:, MAXCOL] + 1) * 365    # maximum age for bin, in days (exclude this value) mask = np.zeros(n_agents, dtype=bool) ages = np.zeros(n_agents, dtype=np.int32) for i in range(len(america)):   # for each possible bin value...     mask[:] = samples == i      # ...find the agents that belong to this bin     # ...and assign a random age, in days, within the bin     ages[mask] = np.random.randint(bin_min_age_days[i], bin_max_age_days[i], mask.sum())  In\u00a0[5]: Copied! <pre>fig, ax1 = plt.subplots()\n\n# Plot histogram of samples on primary y-axis\n\nfraction = america[:, FCOL]/america[:, MCOL:FCOL+1].sum()\nax1.plot(fraction, color=\"g\")\nax1.set_xlabel(\"Age Bin\")\nplt.xticks(rotation=90)\nbins = [f\"{i, j}\" for i, j in zip(america[:, MINCOL], america[:, MAXCOL])]\nax1.set_xticks(ticks=range(len(bins)), labels=bins)\nax1.set_ylim(-0.01, 0.05)\nax1.set_ylabel(\"% Population (Input)\", color=\"g\")\n\n# We will calculate the histogram manually because the final bin isn't the same width as the others.\nhistogram = np.zeros(21, dtype=np.int32)\nfor i in range(histogram.shape[0]):\n    mask = (ages // 1825) == i  # Convert ages in days to 5-year bin indices\n    histogram[i] = mask.sum()\n\n# Create secondary y-axis\nax2 = ax1.twinx()\nscale = america[:,FCOL].sum() / america[:,MCOL:FCOL+1].sum()   # female / male+female\nfraction = (histogram*scale)/histogram.sum()\nax2.plot(range(histogram.shape[0]), fraction, color=\"orange\", marker=\"x\")\nax2.set_ylabel(\"% Population (Sampled)\", color=\"orange\")\nax2.set_ylim(-0.01, 0.05)\n\n\nplt.title(\"Age Distribution in the United States\")\nplt.show()\n</pre> fig, ax1 = plt.subplots()  # Plot histogram of samples on primary y-axis  fraction = america[:, FCOL]/america[:, MCOL:FCOL+1].sum() ax1.plot(fraction, color=\"g\") ax1.set_xlabel(\"Age Bin\") plt.xticks(rotation=90) bins = [f\"{i, j}\" for i, j in zip(america[:, MINCOL], america[:, MAXCOL])] ax1.set_xticks(ticks=range(len(bins)), labels=bins) ax1.set_ylim(-0.01, 0.05) ax1.set_ylabel(\"% Population (Input)\", color=\"g\")  # We will calculate the histogram manually because the final bin isn't the same width as the others. histogram = np.zeros(21, dtype=np.int32) for i in range(histogram.shape[0]):     mask = (ages // 1825) == i  # Convert ages in days to 5-year bin indices     histogram[i] = mask.sum()  # Create secondary y-axis ax2 = ax1.twinx() scale = america[:,FCOL].sum() / america[:,MCOL:FCOL+1].sum()   # female / male+female fraction = (histogram*scale)/histogram.sum() ax2.plot(range(histogram.shape[0]), fraction, color=\"orange\", marker=\"x\") ax2.set_ylabel(\"% Population (Sampled)\", color=\"orange\") ax2.set_ylim(-0.01, 0.05)   plt.title(\"Age Distribution in the United States\") plt.show()"},{"location":"software-overview/demographics/age_pyramid/#age-pyramid-examples","title":"Age pyramid examples\u00b6","text":"<p>We will use a U.S. age pyramid and a Nigerian age pyramid for examples.</p>"},{"location":"software-overview/demographics/age_pyramid/#nigeria","title":"Nigeria\u00b6","text":"<p>Source: https://www.populationpyramid.net/nigeria/2024/</p> <p></p>"},{"location":"software-overview/demographics/age_pyramid/#united-states","title":"United States\u00b6","text":"<p>Source: https://www.populationpyramid.net/united-states-of-america/2024/</p> <p></p>"},{"location":"software-overview/demographics/kmestimator/","title":"Date of non-disease death estimation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\ncumulative = np.array([\n     8131,  9396, 10562, 11636, 12620, 13506, 14287, 14958, 15523, 15997,   # year  0.. 9\n    16400, 16756, 17083, 17401, 17725, 18067, 18437, 18837, 19268, 19726,   # year 10..19\n    20207, 20705, 21215, 21732, 22256, 22785, 23319, 23860, 24407, 24961,   # year 20..29\n    25522, 26091, 26668, 27252, 27845, 28446, 29059, 29684, 30324, 30979,   # year 30..39\n    31649, 32334, 33031, 33737, 34452, 35176, 35913, 36666, 37442, 38247,   # year 40..49\n    39085, 39959, 40869, 41815, 42795, 43811, 44866, 45966, 47118, 48330,   # year 50..59\n    49608, 50958, 52380, 53876, 55442, 57080, 58790, 60574, 62435, 64372,   # year 60..69\n    66380, 68451, 70569, 72719, 74880, 77039, 79179, 81288, 83353, 85355,   # year 70..79\n    87274, 89085, 90766, 92299, 93672, 94884, 95936, 96837, 97594, 98216,   # year 80..89\n    98713, 99097, 99383, 99590, 99735, 99833, 99897, 99939, 99965, 99980,   # year 90..99\n    100000, # year 100+\n], dtype=np.int32)\n</pre> import numpy as np  cumulative = np.array([      8131,  9396, 10562, 11636, 12620, 13506, 14287, 14958, 15523, 15997,   # year  0.. 9     16400, 16756, 17083, 17401, 17725, 18067, 18437, 18837, 19268, 19726,   # year 10..19     20207, 20705, 21215, 21732, 22256, 22785, 23319, 23860, 24407, 24961,   # year 20..29     25522, 26091, 26668, 27252, 27845, 28446, 29059, 29684, 30324, 30979,   # year 30..39     31649, 32334, 33031, 33737, 34452, 35176, 35913, 36666, 37442, 38247,   # year 40..49     39085, 39959, 40869, 41815, 42795, 43811, 44866, 45966, 47118, 48330,   # year 50..59     49608, 50958, 52380, 53876, 55442, 57080, 58790, 60574, 62435, 64372,   # year 60..69     66380, 68451, 70569, 72719, 74880, 77039, 79179, 81288, 83353, 85355,   # year 70..79     87274, 89085, 90766, 92299, 93672, 94884, 95936, 96837, 97594, 98216,   # year 80..89     98713, 99097, 99383, 99590, 99735, 99833, 99897, 99939, 99965, 99980,   # year 90..99     100000, # year 100+ ], dtype=np.int32) In\u00a0[2]: Copied! <pre>from laser_core.demographics import KaplanMeierEstimator\n\nestimator = KaplanMeierEstimator(cumulative)\nnagents = 100_000\ndobs = np.zeros(nagents)    # dates of birth, newborns = 0\ndods = estimator.predict_age_at_death(dobs, max_year=100)   # dates of death in days\n</pre> from laser_core.demographics import KaplanMeierEstimator  estimator = KaplanMeierEstimator(cumulative) nagents = 100_000 dobs = np.zeros(nagents)    # dates of birth, newborns = 0 dods = estimator.predict_age_at_death(dobs, max_year=100)   # dates of death in days In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nhistogram = np.zeros(cumulative.shape[0]+1, np.int32)\nyods = dods // 365  # years of death\nfor i in range(cumulative.shape[0]+1):\n    histogram[i] = np.sum(yods == i)\n\nfig, ax1 = plt.subplots()\n\ncolor = \"tab:orange\"\nax1.set_xlabel(\"Age\")\nax1.set_ylabel(\"Cumulative Sampled Deaths\", color=color)\nax1.plot(np.cumsum(histogram), color=color, marker=\"x\")\nax1.tick_params(axis=\"y\", labelcolor=color)\n\nax2 = ax1.twinx()\ncolor = \"tab:green\"\nax2.set_ylabel(\"Input Cumulative Deaths\", color=color)\nax2.plot(cumulative, color=color)\nax2.tick_params(axis=\"y\", labelcolor=color)\n\nfig.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt  histogram = np.zeros(cumulative.shape[0]+1, np.int32) yods = dods // 365  # years of death for i in range(cumulative.shape[0]+1):     histogram[i] = np.sum(yods == i)  fig, ax1 = plt.subplots()  color = \"tab:orange\" ax1.set_xlabel(\"Age\") ax1.set_ylabel(\"Cumulative Sampled Deaths\", color=color) ax1.plot(np.cumsum(histogram), color=color, marker=\"x\") ax1.tick_params(axis=\"y\", labelcolor=color)  ax2 = ax1.twinx() color = \"tab:green\" ax2.set_ylabel(\"Input Cumulative Deaths\", color=color) ax2.plot(cumulative, color=color) ax2.tick_params(axis=\"y\", labelcolor=color)  fig.tight_layout() plt.show() In\u00a0[4]: Copied! <pre>nagents = 100_000 - cumulative[49]          # how many 50+ years old do we expect?\ndobs = np.full(nagents, 365*50)             # dates of birth, 50 years old\ndobs += np.random.randint(0, 365, nagents)  # add random days\ndods = estimator.predict_age_at_death(dobs, max_year=100)   # dates of death in days\n</pre> nagents = 100_000 - cumulative[49]          # how many 50+ years old do we expect? dobs = np.full(nagents, 365*50)             # dates of birth, 50 years old dobs += np.random.randint(0, 365, nagents)  # add random days dods = estimator.predict_age_at_death(dobs, max_year=100)   # dates of death in days In\u00a0[5]: Copied! <pre>histogram = np.zeros(cumulative.shape[0]+1, np.int32)\nyods = dods // 365  # years of death\nfor i in range(cumulative.shape[0]+1):\n    histogram[i] = np.sum(yods == i)\n\nfig, ax1 = plt.subplots()\n\ncolor = \"tab:orange\"\nax1.set_xlabel(\"Age\")\nax1.set_ylabel(\"Cumulative Sampled Deaths\", color=color)\nax1.plot(np.cumsum(histogram), color=color, marker=\"x\")\nax1.tick_params(axis=\"y\", labelcolor=color)\n\nax2 = ax1.twinx()\ncolor = \"tab:green\"\nax2.set_ylabel(\"Input Cumulative Deaths (50+)\", color=color)\nsubsequent = np.maximum(cumulative - cumulative[49], 0)\nax2.plot(subsequent, color=color)\nax2.tick_params(axis=\"y\", labelcolor=color)\n\nfig.tight_layout()\nplt.show()\n</pre> histogram = np.zeros(cumulative.shape[0]+1, np.int32) yods = dods // 365  # years of death for i in range(cumulative.shape[0]+1):     histogram[i] = np.sum(yods == i)  fig, ax1 = plt.subplots()  color = \"tab:orange\" ax1.set_xlabel(\"Age\") ax1.set_ylabel(\"Cumulative Sampled Deaths\", color=color) ax1.plot(np.cumsum(histogram), color=color, marker=\"x\") ax1.tick_params(axis=\"y\", labelcolor=color)  ax2 = ax1.twinx() color = \"tab:green\" ax2.set_ylabel(\"Input Cumulative Deaths (50+)\", color=color) subsequent = np.maximum(cumulative - cumulative[49], 0) ax2.plot(subsequent, color=color) ax2.tick_params(axis=\"y\", labelcolor=color)  fig.tight_layout() plt.show()"},{"location":"software-overview/demographics/kmestimator/#date-of-non-disease-death-estimation","title":"Date of non-disease death estimation\u00b6","text":"<p>The Kaplan-Meier estimator uses survival information to draw for a date of death based on current age.</p> <p>Current age may be non-zero for an initial population or zero for newborn agents.</p>"},{"location":"software-overview/demographics/kmestimator/#population-of-100000","title":"Population of 100,000\u00b6","text":"<p>The input to the Kaplan-Meier estimator is the cumulative number of deaths by bin/age.</p> <p>The following data from Nigeria follow a hypothetical 100,000-person population giving the cumulative number of deaths by each age.</p>"},{"location":"software-overview/demographics/kmestimator/#population-of-50000","title":"Population of 50,000\u00b6","text":"<p>The input to the Kaplan-Meier estimator is the cumulative number of deaths by bin/age.</p> <p>The following data from Nigeria follow a hypothetical 50,000-person population giving the cumulative number of deaths by each age.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/sir/","title":"Build SIR models","text":"<p>One of the simplest and most commonly used models to describe the progression of an outbreak or epidemic is the SIR (Susceptible - Infected - Recovered) model. We can use the SIR model to explore how to use the LASER framework, staring with a basic SIR model and adding complexity.</p> <p>This tutorial will:</p> <ul> <li>Demonstrate how the <code>LASERframe</code> and <code>PropertySet</code> libraries are used</li> <li>Structure a basic disease transmission framework</li> <li>Track and visualize results</li> </ul> <p>As you progress through the sections, you will learn how to add spatial dynamics and migration into the disease model, using both synthetic and real-world data.</p>"},{"location":"tutorials/sir/#simple-sir","title":"Simple SIR","text":"<p>The SIR model presented here simulates disease dynamics within a closed population in a single node using the <code>LASERFrame</code> framework. The population starts with a defined number of susceptible and infected individuals, progresses over time with recovery and transmission components, and tracks results for visualization. This example serves as a practical guide for modeling simple epidemic dynamics. This simple example does not include vital dynamics, age-structured populations, vaccination, or other complex interactions.</p>"},{"location":"tutorials/sir/#model-components","title":"Model components","text":"<p>The <code>SIRModel</code> class is the core of the implementation. It initializes a population using <code>LaserFrame</code>, sets up disease state and recovery timer properties, and tracks results across timesteps.</p> Code example: Implementing <code>SIRModel</code> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom laser_core import LaserFrame\nfrom laser_core import PropertySet\n\nclass SIRModel:\n    def __init__(self, params):\n        # Model Parameters\n        self.params = params\n\n        # Initialize the population LaserFrame\n        self.population = LaserFrame(capacity=params.population_size,initial_count=params.population_size)\n\n        # Add disease state property (0 = Susceptible, 1 = Infected, 2 = Recovered)\n        self.population.add_scalar_property(\"disease_state\", dtype=np.int32, default=0)\n\n        # Add a recovery timer property (for intrahost progression, optional for timing)\n        self.population.add_scalar_property(\"recovery_timer\", dtype=np.int32, default=0)\n\n        # Results tracking\n        self.results = LaserFrame( capacity = 1 ) # number of nodes\n        self.results.add_vector_property( \"S\", length=params[\"timesteps\"], dtype=np.float32 )\n        self.results.add_vector_property( \"I\", length=params[\"timesteps\"], dtype=np.float32 )\n        self.results.add_vector_property( \"R\", length=params[\"timesteps\"], dtype=np.float32 )\n\n        # Components\n        self.components = []\n\n    def add_component(self, component):\n        self.components.append(component)\n\n    def track_results(self, tick):\n        susceptible = (self.population.disease_state == 0).sum()\n        infected = (self.population.disease_state == 1).sum()\n        recovered = (self.population.disease_state == 2).sum()\n        total = self.population.count\n        self.results.S[tick] = susceptible / total\n        self.results.I[tick] = infected / total\n        self.results.R[tick] = recovered / total\n\n    def run(self):\n        for tick in range(self.params.timesteps):\n            for component in self.components:\n                component.step()\n            self.track_results(tick)\n\n    def plot_results(self):\n        plt.figure(figsize=(10, 6))\n        plt.plot(self.results.S, label=\"Susceptible (S)\", color=\"blue\")\n        plt.plot(self.results.I, label=\"Infected (I)\", color=\"red\")\n        plt.plot(self.results.R, label=\"Recovered (R)\", color=\"green\")\n        plt.title(\"SIR Model Dynamics with LASER Components\")\n        plt.xlabel(\"Time (Timesteps)\")\n        plt.ylabel(\"Fraction of Population\")\n        plt.legend()\n        plt.grid()\n        plt.show()\n        plt.savefig(\"gpt_sir.png\")\n</code></pre> <p>The <code>IntrahostProgression</code> class manages recovery dynamics by updating infected individuals based on a given recovery rate.</p> Code example: Implementing <code>IntrahostProgression</code> <pre><code>class IntrahostProgression:\n    def __init__(self, model):\n        self.population = model.population\n\n        # Seed the infection\n        num_initial_infected = int(0.01 * params.population_size)  # e.g., 1% initially infected\n        infected_indices = np.random.choice(params.population_size, size=num_initial_infected, replace=False)\n        self.population.disease_state[infected_indices] = 1\n\n        # Initialize recovery timer for initially infected individuals\n        initially_infected = self.population.disease_state == 1\n        self.population.recovery_timer[initially_infected] = np.random.randint(5, 15, size=initially_infected.sum())\n\n    def step(self):\n        infected = self.population.disease_state == 1\n\n        # Decrement recovery timer\n        self.population.recovery_timer[infected] -= 1\n\n        # Recover individuals whose recovery_timer has reached 0\n        recoveries = infected &amp; (self.population.recovery_timer &lt;= 0)\n        self.population.disease_state[recoveries] = 2\n</code></pre> <p>The <code>Transmission</code> class manages disease spread by modeling interactions between susceptible and infected individuals.</p> Code example: Implementing <code>Transmission</code> <pre><code>class Transmission:\n    def __init__(self, model):\n        self.population = model.population\n        self.infection_rate = model.params.infection_rate\n\n    def step(self):\n        susceptible = self.population.disease_state == 0\n        infected = self.population.disease_state == 1\n\n        num_susceptible = susceptible.sum()\n        num_infected = infected.sum()\n        population_size = len(self.population)\n\n        # Fraction of infected and susceptible individuals\n        fraction_infected = num_infected / population_size\n\n        # Transmission logic: Probability of infection per susceptible individual\n        infection_probability = self.infection_rate * fraction_infected\n\n        # Apply infection probability to all susceptible individuals\n        new_infections = np.random.rand(num_susceptible) &lt; infection_probability\n\n        # Set new infections and initialize their recovery_timer\n        susceptible_indices = np.where(susceptible)[0]\n        newly_infected_indices = susceptible_indices[new_infections]\n        self.population.disease_state[newly_infected_indices] = 1\n        self.population.recovery_timer[newly_infected_indices] = np.random.randint(5, 15, size=newly_infected_indices.size)  # Random recovery time\n</code></pre> <p>The simulation parameters are defined using the <code>PropertySet</code> class.</p> Code example: Defining simulation parameters using <code>PropertySet</code> <pre><code>params = PropertySet({\n    \"population_size\": 100_000,\n    \"infection_rate\": 0.3,\n    \"timesteps\": 160\n})\n</code></pre> <p>The model is initialized with the defined parameters, components are added, and the simulation is run for the specified timesteps. Results are then visualized.</p> Code example: Intiailize, run the simulation, and plot the results <pre><code># Initialize the model\nsir_model = SIRModel(params)\n\n# Initialize and add components\nsir_model.add_component(IntrahostProgression(sir_model))\nsir_model.add_component(Transmission(sir_model))\n\n# Run the simulation\nsir_model.run()\n\n# Plot results\nsir_model.plot_results()\n</code></pre>"},{"location":"tutorials/sir/#spatial-sir","title":"Spatial SIR","text":"<p>Building upon the simple SIR  model created above, we can add spatial complexity to the framework. Here the simple SIR model will spread the population across 20 nodes. The nodes are arranged in a 1-dimensional chain and infection spreads spatially from node 0 as agents migrate; migration is based on a migration matrix.</p> <p>In this example, two migration options are available:</p> <ol> <li>Sequential migration matrix: Agents can only move to the next node in the chain.</li> <li>Gravity model migration matrix: Agents can move in a 2-dimensional spatial dynamic, where migration probabilities depend on node distances and population sizes.</li> </ol> <p>For this example, the population is distributed across nodes using a rural-urban skew, and migration timers are assigned to control agent migration frequency.</p>"},{"location":"tutorials/sir/#model-components_1","title":"Model components","text":"<p>As above, the model will require the use of <code>LaserFrame</code>, but will now also include spatial components.</p> Code example: Initial model importations <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nfrom laser_core.laserframe import LaserFrame\nfrom laser_core.demographics.spatialpops import distribute_population_skewed as dps\nfrom laser_core.migration import gravity\n</code></pre> <p>Instead of using the <code>SIRModel</code>, we will use the <code>MultiNodeSIRModel</code>.</p> Code example: Creating a model using <code>MultiNodeSIRModel</code> <pre><code># Define the model\nclass MultiNodeSIRModel:\n    \"\"\"\n    A multi-node SIR (Susceptible-Infected-Recovered) disease transmission model.\n\n    Attributes:\n        params (dict): Configuration parameters for the model.\n        nodes (int): Number of nodes in the simulation.\n        population (LaserFrame): Represents the population with agent-level properties.\n        results (np.ndarray): Stores simulation results for S, I, and R per node.\n        components (list): List of components (e.g., Migration, Transmission) added to the model.\n    \"\"\"\n\n    def __init__(self, params):\n        \"\"\"\n        Initializes the SIR model with the given parameters.\n\n        Args:\n            params (dict): Dictionary containing parameters such as population size,\n                           number of nodes, timesteps, and rates for transmission/migration.\n        \"\"\"\n        self.components = []\n        self.params = params\n        self.nodes = params[\"nodes\"]\n        self.population = LaserFrame(capacity=params[\"population_size\"], initial_count=params[\"population_size\"])\n\n        # Define properties\n        self.population.add_scalar_property(\"node_id\", dtype=np.int32)\n        self.population.add_scalar_property(\"disease_state\", dtype=np.int32, default=0)  # 0=S, 1=I, 2=R\n        self.population.add_scalar_property(\"recovery_timer\", dtype=np.int32, default=0)\n        self.population.add_scalar_property(\"migration_timer\", dtype=np.int32, default=0)\n\n        # Initialize population distribution\n        node_pops = dps(params[\"population_size\"], self.nodes, frac_rural=0.3)\n        node_ids = np.concatenate([np.full(count, i) for i, count in enumerate(node_pops)])\n        np.random.shuffle(node_ids)\n        self.population.node_id[:params[\"population_size\"]] = node_ids\n\n        # Reporting structure: Use LaserFrame for reporting\n        self.results = LaserFrame( capacity=self.nodes ) # not timesteps for capacity\n        for state in [\"S\", \"I\", \"R\"]:\n            self.results.add_vector_property(state, length=params[\"timesteps\"], dtype=np.int32)\n\n        # Record results: reporting could actually be a component if we wanted. Or it can be\n        # done in a log/report function in the relevant component (e.g., Transmission)\n        self.results.S[self.current_timestep, :] = [\n            np.sum(self.population.disease_state[self.population.node_id == i] == 0)\n            for i in range(self.nodes)\n        ]\n        self.results.I[self.current_timestep, :] = [\n            np.sum(self.population.disease_state[self.population.node_id == i] == 1)\n            for i in range(self.nodes)\n        ]\n        self.results.R[self.current_timestep, :] = [\n            np.sum(self.population.disease_state[self.population.node_id == i] == 2)\n            for i in range(self.nodes)\n        ]\n\n    def add_component(self, component):\n        \"\"\"\n        Adds a component (e.g., Migration, Transmission, Recovery) to the model.\n\n        Args:\n            component: An instance of a component to be added.\n        \"\"\"\n        self.components.append(component)\n\n    def step(self):\n        \"\"\"\n        Advances the simulation by one timestep, updating all components and recording results.\n        \"\"\"\n        for component in self.components:\n            component.step()\n\n        # Record results\n        for i in range(self.nodes):\n            in_node = self.population.node_id == i\n            self.results[self.current_timestep, i, 0] = (self.population.disease_state[in_node] == 0).sum()\n            self.results[self.current_timestep, i, 1] = (self.population.disease_state[in_node] == 1).sum()\n            self.results[self.current_timestep, i, 2] = (self.population.disease_state[in_node] == 2).sum()\n\n    def run(self):\n        \"\"\"\n        Runs the simulation for the configured number of timesteps.\n        \"\"\"\n        from tqdm import tqdm\n        for self.current_timestep in tqdm(range(self.params[\"timesteps\"])):\n            self.step()\n\n    def save_results(self, filename):\n        \"\"\"\n        Saves the simulation results to a CSV file.\n\n        Args:\n            filename (str): Path to the output file.\n        \"\"\"\n        with open(filename, mode='w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow([\"Timestep\", \"Node\", \"Susceptible\", \"Infected\", \"Recovered\"])\n            for t in range(self.params[\"timesteps\"]):\n                for node in range(self.nodes):\n                    writer.writerow([t, node, *self.results[t, node]])\n\n    def plot_results(self):\n        \"\"\"\n        Plots the prevalence of infected agents over time for all nodes.\n        \"\"\"\n        plt.figure(figsize=(10, 6))\n        for i in range(self.nodes):\n            prevalence = self.results.I[:, i] / (\n                self.results.S[:, i] +\n                self.results.I[:, i] +\n                self.results.R[:, i]\n            )\n            plt.plot(prevalence, label=f\"Node {i}\")\n\n        plt.title(\"Prevalence Across All Nodes\")\n        plt.xlabel(\"Timesteps\")\n        plt.ylabel(\"Prevalence of Infected Agents\")\n        plt.legend()\n        plt.show()\n</code></pre> <p>To add migration between nodes, we will need to select the type of migration model to use and import the component. Here, we will use the sequential migration matrix to move agents sequentially between nodes. The 0th node is the 'urban' node which contains the largest population and where we seed the infection. The infection will travel sequentially from node to node, but the below example breaks the connection at node 13 for demonstrative purposes.</p> Code example: Adding migration using the sequential migration matrix <pre><code>class MigrationComponent:\n    \"\"\"\n    Handles migration behavior of agents between nodes in the model.\n\n    Attributes:\n        model (MultiNodeSIRModel): The simulation model instance.\n        migration_matrix (ndarray): A matrix representing migration probabilities between nodes.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        Initializes the MigrationComponent.\n\n        Args:\n            model (MultiNodeSIRModel): The simulation model instance.\n        \"\"\"\n        self.model = model\n\n        # Set initial migration timers\n        max_timer = int(1 / model.params[\"migration_rate\"])\n        model.population.migration_timer[:] = np.random.randint(1, max_timer + 1, size=model.params[\"population_size\"])\n\n        self.migration_matrix = self.get_sequential_migration_matrix(model.nodes)\n\n        # Example customization: Disable migration from node 13 to 14\n        def break_matrix_node(matrix, from_node, to_node):\n            matrix[from_node][to_node] = 0\n        break_matrix_node(self.migration_matrix, 13, 14)\n\n    def get_sequential_migration_matrix(self, nodes):\n        \"\"\"\n        Creates a migration matrix where agents can only migrate to the next sequential node.\n\n        Args:\n            nodes (int): Number of nodes in the simulation.\n\n        Returns:\n            ndarray: A migration matrix where migration is allowed only to the next node.\n        \"\"\"\n        migration_matrix = np.zeros((nodes, nodes))\n        for i in range(nodes - 1):\n            migration_matrix[i, i + 1] = 1.0\n        return migration_matrix\n\n    def step(self):\n        \"\"\"\n        Updates the migration state of the population by determining which agents migrate\n        and their destinations based on the migration matrix.\n        \"\"\"\n        node_ids = self.model.population.node_id\n\n        # Decrement migration timers\n        self.model.population.migration_timer -= 1\n\n        # Identify agents ready to migrate\n        migrating_indices = np.where(self.model.population.migration_timer &lt;= 0)[0]\n        if migrating_indices.size == 0:\n            return\n\n        # Shuffle migrants and assign destinations based on migration matrix\n        np.random.shuffle(migrating_indices)\n        destinations = np.empty(len(migrating_indices), dtype=int)\n        for origin in range(self.model.nodes):\n            origin_mask = node_ids[migrating_indices] == origin\n            num_origin_migrants = origin_mask.sum()\n\n            if num_origin_migrants &gt; 0:\n                # Assign destinations proportionally to migration matrix\n                destination_counts = np.round(self.migration_matrix[origin] * num_origin_migrants).astype(int)\n                destination_counts = np.maximum(destination_counts, 0)  # Clip negative values\n                if destination_counts.sum() == 0:  # No valid destinations\n                    destinations[origin_mask] = origin  # Stay in the same node\n                    continue\n                destination_counts[origin] += num_origin_migrants - destination_counts.sum()  # Adjust rounding errors\n\n                # Create ordered destination assignments\n                destination_indices = np.repeat(np.arange(self.model.nodes), destination_counts)\n                destinations[origin_mask] = destination_indices[:num_origin_migrants]\n\n        # Update node IDs of migrants\n        node_ids[migrating_indices] = destinations\n\n        # Reset migration timers for migrated agents\n        self.model.population.migration_timer[migrating_indices] = np.random.randint(\n            1, int(1 / self.model.params[\"migration_rate\"]) + 1, size=migrating_indices.size\n        )\n</code></pre> <p>To create more complicated and more realistic migration dynamics, instead of using sequential migration we can use the gravity model to implement 2-D migration. Migration rates are proportional to population sizes, but the example still uses synthetic distances for ease of demonstration.</p> Code example: Adding migration using the gravity model of migration <pre><code>class MigrationComponent2D:\n    \"\"\"\n    Handles migration behavior of agents between nodes in the model.\n\n    Attributes:\n        model (MultiNodeSIRModel): The simulation model instance.\n        migration_matrix (ndarray): A matrix representing migration probabilities between nodes.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        Initializes the MigrationComponent.\n\n        Args:\n            model (MultiNodeSIRModel): The simulation model instance.\n        \"\"\"\n        self.model = model\n\n        # Set initial migration timers\n        max_timer = int(1 / model.params[\"migration_rate\"])\n        model.population.migration_timer[:] = np.random.randint(1, max_timer + 1, size=model.params[\"population_size\"])\n\n        self.migration_matrix = self.get_gravity_migration_matrix(model.nodes)\n\n    def get_gravity_migration_matrix(self, nodes):\n        \"\"\"\n        Generates a gravity-based migration matrix based on population and distances between nodes.\n\n        Args:\n            nodes (int): Number of nodes in the simulation.\n\n        Returns:\n            ndarray: A migration matrix where each row represents probabilities of migration to other nodes.\n        \"\"\"\n        pops = np.array([np.sum(self.model.population.node_id == i) for i in range(nodes)])\n        distances = np.ones((nodes, nodes)) - np.eye(nodes)\n        migration_matrix = gravity(pops, distances, k=1.0, a=0.5, b=0.5, c=2.0)\n        migration_matrix = migration_matrix / migration_matrix.sum(axis=1, keepdims=True)\n        return migration_matrix\n\n    def step(self):\n\n        \"\"\"\n        Executes the migration step for the agent-based model.\n\n        This function selects a fraction of agents to migrate based on expired migration timers.\n        It then changes their node_id according to the migration matrix, ensuring that movements\n        follow the prescribed probability distributions.\n\n        Steps:\n        - Selects a subset of the population for migration.\n        - Determines the origin nodes of migrating agents.\n        - Uses a multinomial draw to assign new destinations based on the migration matrix.\n        - Updates the agents' node assignments accordingly.\n\n        Returns:\n            None\n        \"\"\"\n        # Decrement migration timers\n        self.model.population.migration_timer -= 1\n\n        # Identify agents ready to migrate\n        migrating_indices = np.where(self.model.population.migration_timer &lt;= 0)[0]\n        if migrating_indices.size == 0:\n            return\n\n        np.random.shuffle(migrating_indices)\n\n        origins = model.population.node_id[migrating_indices]\n        origin_counts = np.bincount(origins, minlength=model.params[\"nodes\"])\n\n        offset = 0\n\n        for origin in range(model.params[\"nodes\"]):\n            count = origin_counts[origin]\n            if count == 0:\n                continue\n\n            origin_slice = migrating_indices[offset : offset + count]\n            offset += count\n\n            row = self.migration_matrix[origin]\n            row_sum = row.sum()\n            if row_sum &lt;= 0:\n                continue\n\n            fraction_row = row / row_sum\n            destination_counts = np.random.multinomial(count, fraction_row)\n            destinations = np.repeat(np.arange(model.params[\"nodes\"]), destination_counts)\n            model.population.node_id[origin_slice] = destinations[:count]\n\n        # Reset migration timers for migrated agents\n        self.model.population.migration_timer[migrating_indices] = np.random.randint(\n            1, int(1 / self.model.params[\"migration_rate\"]) + 1, size=migrating_indices.size\n        )\n</code></pre> <p>After selecting your desired migration patterns, you will need to add in a transmission component to create disease dynamics.</p> Code example: Adding in disease transmission <pre><code>class TransmissionComponent:\n    \"\"\"\n    Handles the disease transmission dynamics within the population.\n\n    Attributes:\n        model (MultiNodeSIRModel): The simulation model instance.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        Initializes the TransmissionComponent and infects initial agents.\n\n        Args:\n            model (MultiNodeSIRModel): The simulation model instance.\n        \"\"\"\n        self.model = model\n\n    def step(self):\n        \"\"\"\n        Simulates disease transmission for each node in the current timestep.\n        \"\"\"\n        for i in range(self.model.nodes):\n            in_node = self.model.population.node_id == i\n            susceptible = in_node &amp; (self.model.population.disease_state == 0)\n            infected = in_node &amp; (self.model.population.disease_state == 1)\n\n            num_susceptible = susceptible.sum()\n            num_infected = infected.sum()\n            total_in_node = in_node.sum()\n\n            if total_in_node &gt; 0 and num_infected &gt; 0 and num_susceptible &gt; 0:\n                infectious_fraction = num_infected / total_in_node\n                susceptible_fraction = num_susceptible / total_in_node\n\n                new_infections = int(\n                    self.model.params[\"transmission_rate\"] * infectious_fraction * susceptible_fraction * total_in_node\n                )\n\n                susceptible_indices = np.where(susceptible)[0]\n                newly_infected_indices = np.random.choice(susceptible_indices, size=new_infections, replace=False)\n\n                self.model.population.disease_state[newly_infected_indices] = 1\n                self.model.population.recovery_timer[newly_infected_indices] = np.random.randint(5, 15, size=new_infections)\n</code></pre> <p>Finally, we need to add recovery dynamics to the model to move agents through the disease progression.</p> Code example: Adding in recovery dynamics <pre><code>class RecoveryComponent:\n    \"\"\"\n    Handles the recovery dynamics of infected individuals in the population.\n\n    Attributes:\n        model (MultiNodeSIRModel): The simulation model instance.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        Initializes the RecoveryComponent.\n\n        Args:\n            model (MultiNodeSIRModel): The simulation model instance.\n        \"\"\"\n        self.model = model\n\n    def step(self):\n        \"\"\"\n        Updates the recovery state of infected individuals, moving them to the recovered state\n        if their recovery timer has elapsed.\n        \"\"\"\n        infected = self.model.population.disease_state == 1\n        self.model.population.recovery_timer[infected] -= 1\n        self.model.population.disease_state[(infected) &amp; (self.model.population.recovery_timer &lt;= 0)] = 2\n</code></pre> <p>To run the created model and visualize your output, we will need to set our model parameters and run the simulation.</p> Code example: Running your spatial SIR model <pre><code># Parameters\nparams = {\n    \"population_size\": 1_000_000,\n    \"nodes\": 20,\n    \"timesteps\": 600,\n    \"initial_infected_fraction\": 0.01,\n    \"transmission_rate\": 0.25,\n    \"migration_rate\": 0.001\n}\n\n# Run simulation\nmodel = MultiNodeSIRModel(params)\nmodel.add_component(MigrationComponent(model))\nmodel.add_component(TransmissionComponent(model))\nmodel.add_component(RecoveryComponent(model))\nmodel.run()\nmodel.save_results(\"simulation_results.csv\")\nmodel.plot_results()\n</code></pre>"},{"location":"tutorials/sir/#using-real-data","title":"Using real data","text":"<p>To utilize SIR models to understand real-world transmission dynamics, you will need to use real data. Model structure will be similar to what was presented above, but instead of using a synthetic population we will initialize the model using real population data. In this example, we will use data from Rwanda. You will want your data saved in a .csv file, with the following information:</p> <ul> <li>Region_id: node location, here each node is a city in Rwanda</li> <li>Population: the population of the node</li> <li>Centroid_lat and centroid-long: the latitude and longitude at the center of the node</li> <li>Birth_rate: the birth rate for the node</li> </ul> region_id population centroid_lat centroid_long birth_rate Ryansoro 46482.7 -3.70727 29.7988 11.6565 Ndava 72979.3 -3.39156 29.7534 15.8815 Buyengero 76468.8 -3.84874 29.533 12.5038 Bugarama 44571.9 -3.69048 29.4004 11.0256 Rumonge 300248 -3.96221 29.4571 19.5677 Burambi 63219.7 -3.79864 29.4524 9.19902 Kanyosha1 115018 -3.43097 29.4153 37.9514 Kabezi 71913.8 -3.5311 29.37 31.8319 Muhuta 88141.7 -3.62351 29.4152 21.5989 <p>The model code will be very similar to the code used above, but the population data will be loaded from the .csv file instead of created synthetically. In the following example, numbers are rounded and scaled down (which is optional), and each node is assigned an ID.</p> Code example: Creating a model using data loaded from a CSV <pre><code>class SpatialSIRModelRealData:\n    def __init__(self, params, population_data):\n        \"\"\"\n            Initialize the mode, LASER-style, using the population_data loaded from a csv file (pandas).\n            Create nodes and a migration_matrix based on populations and locations of each node.\n\n        \"\"\"\n        self.params = params\n        # We scale down population here from literal values but this may not be necessary.\n        population_data[\"scaled_population\"] = (population_data[\"population\"] / params[\"scale_factor\"]).round().astype(int)\n        total_population = int(population_data[\"scaled_population\"].sum())\n        print( f\"{total_population=}\" )\n\n        # Set up the properties as before\n        self.population = LaserFrame(capacity=total_population, initial_count=total_population)\n        self.population.add_scalar_property(\"node_id\", dtype=np.int32)\n        self.population.add_scalar_property(\"disease_state\", dtype=np.int32, default=0)\n        self.population.add_scalar_property(\"recovery_timer\", dtype=np.int32, default=0)\n        self.population.add_scalar_property(\"migration_timer\", dtype=np.int32, default=0)\n\n        node_pops = population_data[\"scaled_population\"].values\n        self.params[\"nodes\"] = len(node_pops)\n\n        node_ids = np.concatenate([np.full(count, i) for i, count in enumerate(node_pops)])\n        np.random.shuffle(node_ids)\n        self.population.node_id[:total_population] = node_ids\n\n        # seed in big node\n        big_node_id = np.argmax( node_pops )\n        available_population = population_data[\"scaled_population\"][big_node_id]\n        initial_infected = int(params[\"initial_infected_fraction\"] * available_population)\n        infection_indices = np.random.choice(np.where(self.population.node_id == big_node_id)[0], initial_infected, replace=False)\n        self.population.disease_state[infection_indices] = 1\n        self.population.recovery_timer[infection_indices] = np.random.uniform(params[\"recovery_time\"] - 3, params[\"recovery_time\"] + 3, size=initial_infected).astype(int)\n\n        pop_sizes = np.array(node_pops)\n        latitudes = population_data[\"centroid_lat\"].values\n        longitudes = population_data[\"centroid_long\"].values\n        distances = np.zeros((self.params[\"nodes\"], self.params[\"nodes\"]))\n\n        # Nested for loop here is optimized for readbility, not performance\n        for i in range(self.params[\"nodes\"]):\n            for j in range(self.params[\"nodes\"]):\n                if i != j:\n                    distances[i, j] = distance(latitudes[i], longitudes[i], latitudes[j], longitudes[j])\n\n        # Set up our migration_matrix based on gravity model and input data (pops &amp; distances)\n        self.distances = distances\n        self.migration_matrix = gravity(pop_sizes, distances, k=10.0, a=1.0, b=1.0, c=1.0)\n        self.migration_matrix /= self.migration_matrix.sum(axis=1, keepdims=True) # normalize\n</code></pre>"},{"location":"tutorials/sir/#alternative-migration-approach","title":"Alternative migration approach","text":"<p>In the above examples, we modeled migration by moving individual agents from node to node. An alternative approach is to model the migration of infection instead of individuals; this allows for computational efficiency while maintaining accurate disease transmission dynamics. Note that in this example, we do not use a <code>MigrationComponent</code> or <code>migration_timer</code>.</p> Code example: Modeling migration of infection instead of individuals <pre><code>import numpy as np\nfrom laser_core.migration import gravity\nfrom laser_core.utils import calc_distances\n\nclass TransmissionComponent:\n    \"\"\"\n    Transmission Component\n    =======================\n\n    This class models the transmission of disease using \"infection migration\"\n    instead of human movement. Instead of tracking individual movement,\n    infection is spread probabilistically based on a gravity-inspired network.\n\n    This approach can significantly improve computational efficiency for\n    large-scale spatial epidemic simulations.\n\n    Attributes:\n    ------------\n    model : object\n        The simulation model containing population and node information.\n    network : ndarray\n        A matrix representing the transmission probabilities between nodes.\n    locations : ndarray\n        Array of node latitude and longitude coordinates.\n    \"\"\"\n    def __init__(self, model):\n        \"\"\"\n        Initializes the transmission component by computing the infection migration network.\n\n        Parameters:\n        -----------\n        model : object\n            The simulation model containing population and node information.\n        \"\"\"\n        self.model = model\n        model.nodes.add_vector_property(\"network\", length=model.nodes.count, dtype=np.float32)\n        self.network = model.nodes.network\n\n        # Extract node locations and populations from model.population_data\n        self.locations = np.column_stack((model.population_data[\"centroid_lat\"], model.population_data[\"centroid_long\"]))\n        initial_populations = np.array(model.population_data[\"population\"])\n\n        # Initialize heterogeneous transmission factor per agent (0.5 to 2.0)\n        self.model.population.tx_hetero_factor = np.random.uniform(0.5, 2.0, size=model.population.capacity)\n\n        # Compute the infection migration network based on node populations.\n        a, b, c, k = self.model.params.a, self.model.params.b, self.model.params.c, self.model.params.k\n\n        # Compute all pairwise distances in one call (this speeds up initialization significantly)\n        # from laser_core.migration import gravity, row_normalizer\n        # from laser_core.utils import calc_distances\n        distances = calc_distances(self.locations[:, 0], self.locations[:, 1])\n        self.network = gravity(initial_populations, distances, k, a, b, c)\n        self.network /= np.power(initial_populations.sum(), c)  # Normalize\n        self.network = row_normalizer(self.network, 0.01) # 0.01=max_frac\n\n   def step(self):\n        \"\"\"\n        Simulates disease transmission and infection migration across the network.\n\n        New infections are determined deterministically based on contagion levels and susceptible fraction.\n        \"\"\"\n        contagious_indices = np.where(self.model.population.disease_state == 1)[0]\n        values = self.model.population.tx_hetero_factor[contagious_indices]  # Apply heterogeneity factor\n\n        # Compute contagion levels per node\n        contagion = np.bincount(\n            self.model.population.node_id[contagious_indices],\n            weights=values,\n            minlength=self.model.nodes.count\n        ).astype(np.float64)\n\n        # Apply network-based infection movement\n        transfer = (contagion * self.network).round().astype(np.float64)\n\n        # Ensure net contagion remains positive after movement\n        contagion += transfer.sum(axis=1) - transfer.sum(axis=0)\n        contagion = np.maximum(contagion, 0)  # Prevent negative contagion\n\n        # Infect susceptible individuals in each node deterministically\n        for i in range(self.model.nodes.count):\n            node_population = np.where(self.model.population.node_id == i)[0]\n            susceptible = node_population[self.model.population.disease_state[node_population] == 0]\n\n            if len(susceptible) &gt; 0:\n                # Compute new infections deterministically based on prevalence and susceptible fraction\n                num_new_infections = int(min(len(susceptible), (\n                    self.model.params.transmission_rate * contagion[i] * len(susceptible) / len(node_population)\n                )))\n\n                # Randomly select susceptible individuals for infection\n                new_infected_indices = np.random.choice(susceptible, size=num_new_infections, replace=False)\n                self.model.population.disease_state[new_infected_indices] = 1\n\n                # Assign recovery timers to newly infected individuals\n                self.model.population.recovery_timer[new_infected_indices] = np.random.randint(5, 15, size=num_new_infections)\n</code></pre>"}]}